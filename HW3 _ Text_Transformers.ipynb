{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dPmdF4DDZnm4h2pnb33xECpd2bsMuOyV","timestamp":1717078789441},{"file_id":"1YgqKV5Lp8Xh5XuzsCcnYjwh3KYlEl8zz","timestamp":1717077736822}],"gpuType":"T4","authorship_tag":"ABX9TyNhxb4FTQfkbETUMim4CUL+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Before you start, make sure you use the T4 GPU processor (Runtime->Change Runtime-> T4GPU)**"],"metadata":{"id":"6SVupk1hL5AP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7360x8KIMZ8"},"outputs":[],"source":["#Import drive\n","from google.colab import drive\n","#Mount Google Drive\n","ROOT=\"/content/drive\"\n","drive.mount(ROOT, force_remount=True)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.backends.cudnn as cudnn\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","\n","\n","cudnn.benchmark = True"],"metadata":{"id":"Ysz5y8V4MRSJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Send the code to the GPU"],"metadata":{"id":"S5kEiSTAMatN"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","\n","print(device)"],"metadata":{"id":"jByQW1X7MYau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/dlss24/"],"metadata":{"id":"P44H1yCiOWZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pwd"],"metadata":{"id":"2xruA5Q5OcHl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformers for classification\n","\n","In this HW you will do the following steps:\n","\n","\n","1.   *1 Pts*: Train a transformer model\n","2.   *1 Pts*: Prediction\n","3.   *1 Pts*: Metrics\n","4.   *3 Pts*: Model comparison and discussion\n","\n","\n","\n","**Before you start, make sure you use the T4 GPU processor (Runtime->Change Runtime-> T4GPU)**\n"],"metadata":{"id":"uN1IQii5IaPY"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","df_source_corpus=pd.read_csv('/content/drive/MyDrive/dlss24/source_corpus.csv')\n","df_source_corpus = df_source_corpus.dropna(subset=['text'])"],"metadata":{"id":"h0sqAjxU05vC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *1 Pt*: Train Model"],"metadata":{"id":"ce1cFc_A3YG5"}},{"cell_type":"markdown","source":["\n","> Complete the classification pipeline\n","\n","\n","1.   Split the data correctly for batches of 8 (line 37)\n","2.   Add right code in line 48, 50,51"],"metadata":{"id":"m9gawUcG083t"}},{"cell_type":"code","source":["import torch\n","from transformers import DistilBertTokenizer, TextClassificationPipeline, DistilBertForSequenceClassification, DistilBertTokenizerFast\n","from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# Load the dataset\n","df_sample = df_source_corpus[0:1000]\n","df_sample = df_sample.assign(topic_id=(df_sample['topic_8']).astype('category').cat.codes)\n","\n","# Determine the number of unique classes\n","num_classes = len(set(df_sample['topic_8'].tolist()))\n","print(f\"Number of classes: {num_classes}\")\n","\n","model_name = 'distilbert-base-uncased'\n","model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n","tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n","\n","# Prepare inputs and labels\n","inputs = tokenizer(df_sample['text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n","labels = torch.tensor(df_sample['topic_id'].tolist()).long()\n","\n","optimizer = torch.optim.Adam([\n","    {'params': model.distilbert.parameters(), 'lr': 1e-5},\n","    {'params': model.classifier.parameters(), 'lr': 1e-3}\n","])\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(df_sample['text'].tolist(), df_sample['topic_id'].tolist(), test_size=.2)\n","\n","# Generate batches\n","X_train, X_test, y_train, y_test = np.array(X_train[:344]), np.array(X_test[:80]), np.array(y_train[:344]), np.array(y_test[:80])\n","\n","# TODO:  BATCHES OF 8\n","X_train, X_test, y_train, y_test = ...\n","\n","\n","X_train, X_test = X_train.tolist(), X_test.tolist()\n","\n","# Train the model\n","model.to('cuda:0')\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    model.train()\n","    for text, labels in tqdm(zip(X_train, y_train), total=len(X_train)):\n","      #TODOD\n","        model_inputs = ...\n","        model_inputs = {k: v.to('cuda:0') for k, v in model_inputs.items()}\n","      #TODO\n","        labels = ...\n","        output = ...\n","        loss, logits = output[:2]\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","torch.save(model, 'topic_classifier.pt')\n","topic_classifier = model\n","\n","distil_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', max_length=512, truncation=True)\n","pipeline = TextClassificationPipeline(model=topic_classifier, tokenizer=distil_tokenizer, return_all_scores=True, device=0)\n","\n","\n","\n","\n","\n"],"metadata":{"id":"PZsXAQGs1AQt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *1 Pt*: Prediction"],"metadata":{"id":"lpww_NndJZ3a"}},{"cell_type":"markdown","source":["> Do predictions on Test dataset"],"metadata":{"id":"lXm04W-d1Io1"}},{"cell_type":"code","source":["# Tokenize and predict on the test set\n","y_true = []\n","y_pred = []\n","\n","for text, true_label in zip(X_test, y_test):\n","    model_inputs = distil_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n","    model_inputs = {k: v.to('cuda:0') for k, v in model_inputs.items()}\n","\n","    with torch.no_grad():\n","       #TODO:\n","        output = ...\n","        logits = ...\n","        predictions = ...\n","\n","    y_true.extend(true_label)\n","    y_pred.extend(predictions)"],"metadata":{"id":"Gskxlemv1F2b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *1 Pt*: Metrics"],"metadata":{"id":"dmKPiJqm3OIU"}},{"cell_type":"markdown","source":["> Create the metrics"],"metadata":{"id":"7tkEWBEh1Qz1"}},{"cell_type":"code","source":["category_mapping = {\n","    'economy': 0,\n","    'politics': 1,\n","    'society': 2,\n","    'freedom and democracy': 3,\n","    'external relations': 4,\n","    'fabric of society': 5,\n","    'welfare and quality of life': 6,\n","    'no topic': 7,\n","}\n","\n","\n","\n","\n","# Ensure y_true and y_pred are in the correct shape\n"," #TODO:\n","y_true = ...\n","y_pred = ..\n","\n","\n","\n","# Generate classification report\n","class_report = ...\n","print(\"Classification Report:\")\n","print(class_report)\n"],"metadata":{"id":"tY3Idq-51NJG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## *3 Pts*: Model comparison and discussion"],"metadata":{"id":"CcKJmmUg3AvV"}},{"cell_type":"markdown","source":["\n","\n","> Train other transformer model on the same dataset and compare the metrics. Discuss their performance\n","\n"],"metadata":{"id":"lBMSFZYI2yAd"}},{"cell_type":"code","source":[],"metadata":{"id":"GlzsRqfU2wXl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jIa1XUGG2v9r"},"execution_count":null,"outputs":[]}]}