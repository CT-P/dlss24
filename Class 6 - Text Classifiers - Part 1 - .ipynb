{"cells":[{"cell_type":"markdown","metadata":{"id":"lk20aZuElAuh"},"source":["(from last class) Real world example\n","\n","https://github.com/pytorch/examples/blob/main/imagenet/main.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcDm6T3HaRYG"},"outputs":[],"source":["#Import drive\n","from google.colab import drive\n","#Mount Google Drive\n","ROOT=\"/content/drive\"\n","drive.mount(ROOT, force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Zxub8uiRF7M"},"outputs":[],"source":["%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajyqZyUQRLdL"},"outputs":[],"source":["%cd /content/drive/MyDrive/dlss24"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTf-v1gWRNv2"},"outputs":[],"source":["%pwd"]},{"cell_type":"markdown","metadata":{"id":"4lnmzacMRV9e"},"source":["# Data\n","\n","Download the corpora data from: https://codeocean.com/capsule/0078777/tree/v1\n","\n","Uploaded to a location outside your gitHub (to do not be tracked)\n","\n","You pwd should have the files:\n","\n","\n","1.   source_corpus.csv\n","2.   target_corpus.csv\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BE3M-5mHbRdU"},"source":["\n","# Module IV:  - Class 6 - Text Classifiers\n","\n","This notebook have the network framework pytorch and how CNNs work"]},{"cell_type":"markdown","metadata":{"id":"T5LxOrR6bjFY"},"source":["# Basic Pre processing and tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmnDoKifpFZ7"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eOaGLl-Lbxjf"},"outputs":[],"source":["#load the pickle file in OLAT\n","#df_dummies_model.to_pickle('df_dummies_model.pkl')\n","\n","df_source_corpus=pd.read_csv('/content/drive/MyDrive/dlss24/source_corpus.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1JiJSClKR2M"},"outputs":[],"source":["df_source_corpus.head()"]},{"cell_type":"markdown","metadata":{"id":"0LA4tL-rg1-O"},"source":["\n","\n","> Create a plot with the number of sentences (or rows) per topic\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AMXlfJthAm7"},"outputs":[],"source":["#### YOUR CODE STARTS HERE ####\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","...\n","\n","# Add labels and title if needed\n","plt.xlabel('Count')\n","plt.ylabel('Topic')\n","plt.title('Number of Sentences per Topic')\n","\n","# Show the plot\n","plt.show()\n","\n","\n","##### YOUR CODE ENDS HERE ####"]},{"cell_type":"markdown","metadata":{"id":"aceUbR3zUuVR"},"source":["## Sentence Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOJ23UjupL5u"},"outputs":[],"source":["text = \"K. Alex MÃ¼ller on the right track: Soon afterwards, he and M.A's J. Georg Bednorz discovered the first high-temperature superconductor. The 2 are nobel prize winners.\""]},{"cell_type":"markdown","metadata":{"id":"p4Rs-OPsVKUL"},"source":["**NLTK**: fast with some errors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeMqtEnAhZdm"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","from nltk import sent_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dkD2rd1K-A7"},"outputs":[],"source":["# by puntuation\n","sentences = sent_tokenize(text) # split document into sentences\n","print(sentences)"]},{"cell_type":"markdown","metadata":{"id":"90g9OtsTVRBl"},"source":["**Spacy**: better than NLTK"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PYqJHpdhf6R"},"outputs":[],"source":["import spacy\n","# by language\n","nlp = spacy.load('en_core_web_sm')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K64syxonk_OF"},"outputs":[],"source":["doc = nlp(text)\n","sentences = list(doc.sents)\n","print(sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQIHpFeshsia"},"outputs":[],"source":["doc.is_sentenced"]},{"cell_type":"markdown","metadata":{"id":"vI3aBVkIVYn9"},"source":["# Pre Processing\n","\n","\n","\n","0.   Choose the unit (sentence, document, paragraph, speech)\n","\n","1.   Capitalization\n","2.   Puntuation\n","3.   Tokens\n","4.   Numbers\n","5.   Stopwords\n","6.   Stemming\n","7.   Lemmatizing\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8c0d699mjVAt"},"source":["### 1. Capitalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eW2KQ-Vhk_Kq"},"outputs":[],"source":["# Capitalization\n","text_lower = text.lower() # go to lower-case"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zIcnxWUIVhmA"},"outputs":[],"source":["text_lower"]},{"cell_type":"markdown","metadata":{"id":"1dppmw9_jaO8"},"source":["### 2. Punctuation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c54ZmC35iS9-"},"outputs":[],"source":["# recipe for fast punctuation removal\n","from string import punctuation\n","print (\"punctuation:\", punctuation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4jUXfILVgyx"},"outputs":[],"source":["#maktrans function from str, args are: find-character, replace-character, remove-character\n","punc_remover = str.maketrans('','',punctuation)\n","text_nopunc = text_lower.translate(punc_remover)\n","print(text_nopunc)"]},{"cell_type":"markdown","metadata":{"id":"BI316WRxkht7"},"source":["### 3. Tokenization of words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1013LQkVjov"},"outputs":[],"source":["# Tokens\n","tokens = text_nopunc.split() # splits a string on white space\n","print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"-TkmGJydkxsS"},"source":["### 4. Numbers"]},{"cell_type":"markdown","metadata":{"id":"-1F60q--i9Lc"},"source":["\n","\n","> Replace the number found by the # character\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbU_X5IkjNh6"},"outputs":[],"source":["#### YOUR CODE STARTS HERE ####\n","\n","\n","\n","# keep if not a digit, else replace with \"#\"\n","norm_numbers = ...\n","print(no_numbers )\n","print(norm_numbers)\n","\n","\n","\n","##### YOUR CODE ENDS HERE ####"]},{"cell_type":"markdown","metadata":{"id":"2L4ZtOCQjjzp"},"source":["### 5. Stopwords"]},{"cell_type":"markdown","source":["From NLTK"],"metadata":{"id":"lGkrp9P89OPB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y55Qorfej1k3"},"outputs":[],"source":["# Stopwords\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEsf3cpsj3aj"},"outputs":[],"source":["stoplist = stopwords.words('english')\n","print (\"stop words:\", stoplist)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnEUQNSWVu4M"},"outputs":[],"source":["# keep if not a stopword\n","nostop = [t for t in norm_numbers if t not in stoplist]\n","print(nostop)"]},{"cell_type":"markdown","source":["From scikit-learn"],"metadata":{"id":"8eZAiCcq9Ptz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhtfQ9X6V7PL"},"outputs":[],"source":["# scikit-learn stopwords\n","# depending on sklearn version, for sklearn==0.24.1, stop_words are here\n","from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS as stop_words\n","sorted(list(stop_words))[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfOlO-3CWDcZ"},"outputs":[],"source":["# spacy stopwords\n","sorted(list(nlp.Defaults.stop_words))[:20]"]},{"cell_type":"markdown","metadata":{"id":"fuA_VrFJkGy3"},"source":["### 6. Stemming\n","\n","Heuristic based, reduce variations of words like -ed, -s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"flULQprzkGh7"},"outputs":[],"source":["# Stemming\n","from nltk.stem import SnowballStemmer\n","stemmer = SnowballStemmer('english') # snowball stemmer, english"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hcjO0MTWGyM"},"outputs":[],"source":["# remake list of tokens, replace with stemmed versions\n","tokens_stemmed = [stemmer.stem(t) for t in tokens]\n","print(tokens_stemmed)"]},{"cell_type":"markdown","metadata":{"id":"hPo5zOX2kMex"},"source":["### 7. Lemmatizing\n","\n","Linguistic-based, reduces words with linguistic meaning. Superlatives and Verbs infinitive/base form."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfA6RpZCkMK_"},"outputs":[],"source":["# Lemmatizing\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4e0AUpTlWKRu"},"outputs":[],"source":["wnl = WordNetLemmatizer()\n","wnl.lemmatize('corporation'), wnl.lemmatize('corporations')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwKiDkeGlOuF"},"outputs":[],"source":["stemmer.stem('corporations')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpHA7ZkPlOX2"},"outputs":[],"source":["wnl.lemmatize('was')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FbT2LTIlh_o"},"outputs":[],"source":["wnl.lemmatize('went') #go"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZ_oDZQYluio"},"outputs":[],"source":["wnl.lemmatize('better') #good"]},{"cell_type":"markdown","metadata":{"id":"tw8nNiH8WXZx"},"source":["All in one function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTl6RmQ3WOmf"},"outputs":[],"source":["from string import punctuation\n","translator = str.maketrans('','',punctuation)\n","from nltk.corpus import stopwords\n","stoplist = set(stopwords.words('english'))\n","from nltk.stem import SnowballStemmer\n","stemmer = SnowballStemmer('english')\n","\n","def normalize_text(doc):\n","    \"Input doc and return clean list of tokens\"\n","    doc = doc.replace('\\r', ' ').replace('\\n', ' ')\n","    lower = doc.lower() # all lower case\n","    nopunc = lower.translate(translator) # remove punctuation\n","    words = nopunc.split() # split into tokens\n","    nostop = [w for w in words if w not in stoplist] # remove stopwords\n","    no_numbers = [w if not w.isdigit() else '#' for w in nostop] # normalize numbers\n","    stemmed = [stemmer.stem(w) for w in no_numbers] # stem each word\n","    return stemmed"]},{"cell_type":"markdown","metadata":{"id":"FOJhxGT8mN0D"},"source":["\n","\n","> Apply the normalized_text function to the text column of df_source_corpus\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bl4waJqqmaAN"},"outputs":[],"source":["#### YOUR CODE STARTS HERE ####\n","\n","\n","df_source_corpus['tokens_cleaned'] = ...\n","\n","\n","\n","\n","##### YOUR CODE ENDS HERE ####"]},{"cell_type":"markdown","metadata":{"id":"rgCC0QgMXK6F"},"source":["**Gensim**: All in one"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6p_lIeHHXGCP"},"outputs":[],"source":["from gensim.utils import simple_preprocess # lowercase, tokenized, punctuations/numbers removed\n","print(simple_preprocess(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"457Z-YyTXPvS"},"outputs":[],"source":["# I have cleaned 'text' colun before\n","df_source_corpus['tokens_cleaned'] = df_source_corpus['text'].apply(simple_preprocess)\n","df_source_corpus['tokens_cleaned']"]},{"cell_type":"markdown","metadata":{"id":"RmyojdMYXdXV"},"source":["(OPTIONAL) If you want to customize the gensim:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ff1kyyRzXY-5"},"outputs":[],"source":["complicated_text = \"<div>Prof. Zurich <i>hailed</i> from Zurich., She got 3 M.A.'s from ETH.</div>\" # added html tags\n","\n","from gensim.parsing.preprocessing import preprocess_string, \\\n","strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short, stem_text\n","\n","preprocess_string(complicated_text) # with all default filters: lowercase, tags, puncations, whitespaces, numerics, short words, stop words.\n","\n","# remove some filters to keep more tokens.\n","# e.g. If I want to keep stopwords, short words, and numbers and keep the words not stemmed\n","CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces]\n","preprocess_string(complicated_text, CUSTOM_FILTERS) # only use the customized filters"]},{"cell_type":"markdown","metadata":{"id":"fouG1yxRXueZ"},"source":["# Full analysis with Spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgXK_oiGXwjh"},"outputs":[],"source":["dfs = df_source_corpus.sample(1000)\n","dfs['doc'] = dfs['text'].apply(nlp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGtVeni5p3u_"},"outputs":[],"source":["dfs=dfs.reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTNJlDSImwei"},"outputs":[],"source":["# The spacy model already gives you sentences and tokens.\n","# For example:\n","sent1 = list(dfs['doc'].iloc[0].sents)[0]\n","sent1 # sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BNZzz60m3hA"},"outputs":[],"source":["# tokens\n","list(sent1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNJ7EFu3m561"},"outputs":[],"source":["# lemmas\n","[x.lemma_ for x in sent1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaQjLD-gm9kG"},"outputs":[],"source":["# POS tags\n","[x.tag_ for x in sent1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pY4P_xfvry6D"},"outputs":[],"source":["\n","from collections import Counter\n","# Initialize a Counter to count tokens\n","token_counter = Counter()\n","\n","# Iterate over each Doc object in the 'doc' column and count tokens\n","for doc in dfs['doc']:\n","    token_counter.update([token.text for token in doc])\n","\n","# Print the token counts\n","print(token_counter)"]},{"cell_type":"code","source":[],"metadata":{"id":"cJSZW9mIA-yK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4RPyMlGCr43J"},"outputs":[],"source":["token_counter.most_common()[:20]"]},{"cell_type":"markdown","source":["\n","\n","> Print the 20 least common tokens\n","\n","\n"],"metadata":{"id":"Y6s-4OtI_4Au"}},{"cell_type":"code","source":["#### YOUR CODE STARTS HERE ####\n","\n","\n"," ...\n","\n","\n","\n","\n","##### YOUR CODE ENDS HERE ####"],"metadata":{"id":"bSf6kfzf_3sa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IQSMsW0TEHwF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_99nka5nB4G"},"outputs":[],"source":["dfs['num_words'] = dfs['doc'].apply(lambda x: len(list(x)))\n","dfs['num_sents'] = dfs['doc'].apply(lambda x: len(list(x.sents)))\n","\n","\n","print(len(token_counter),'words in corpus.')\n","words_per_sent = len(token_counter) / len(dfs['doc'])\n","print(words_per_sent,'words per sentence.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewiR1d2Mqo3A"},"outputs":[],"source":["dfs"]},{"cell_type":"markdown","source":["[CountVectorizer Doc](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"],"metadata":{"id":"qO5vuBlDBTy_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8cLjUsznTP5"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","vec = CountVectorizer(min_df=0.01, # at min 1% of docs\n","                        max_df=.9,\n","                        max_features=1000,\n","                        stop_words='english',\n","                        ngram_range=(1,3))\n","X = vec.fit_transform(dfs['text'])\n","\n","# save the vectors\n","#pd.to_pickle(X,'X.pkl')\n","\n"]},{"cell_type":"markdown","source":["\n","\n","> Print the shape of the Count Vectorizer and one of its samples\n","\n"],"metadata":{"id":"_DfbsVliFysX"}},{"cell_type":"code","source":["#### YOUR CODE STARTS HERE ####\n","\n","\n"," ...\n","\n","\n","\n","\n","##### YOUR CODE ENDS HERE ####"],"metadata":{"id":"LqqkoL2wGCko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9U1NmJS1GImr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select tokens with counts greater than 300\n","selected_tokens = {token: count for token, count in token_counter.items() if 12< count < 90 }\n"],"metadata":{"id":"P7aDBJtzETsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(selected_tokens)"],"metadata":{"id":"zXbWSZVgEW06"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency.\n"],"metadata":{"id":"lmaBa1XIGwDB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTblS3EHnYf9"},"outputs":[],"source":["# tf-idf vectorizer up-weights rare/distinctive words\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf = TfidfVectorizer(min_df=0.01,\n","                        max_df=0.9,\n","                        max_features=1000,\n","                        stop_words='english',\n","                        use_idf=True, # the new piece\n","                        ngram_range=(1,2),\n","                        smooth_idf=True)\n","\n","X_tfidf = tfidf.fit_transform(dfs['text'])\n","#pd.to_pickle(X_tfidf,'X_tfidf.pkl')"]},{"cell_type":"code","source":["X_tfidf[0].toarray()"],"metadata":{"id":"au4PQgQtEv9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qg87tIDanfDt"},"outputs":[],"source":["dfs['topic_8'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOD0P0u-ni-y"},"outputs":[],"source":["vocab = tfidf.get_feature_names_out()\n","vocab[:10], vocab[-10:]"]},{"cell_type":"markdown","source":["### Word Cloud"],"metadata":{"id":"GZb0zT1CMOg5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rn3cclItnlO1"},"outputs":[],"source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","for topic_id in ['welfare and quality of life','economy','social groups','political system']:\n","    slicer = dfs['topic_8'] == topic_id\n","\n","    #selecting rows\n","    f = X_tfidf[slicer.values]\n","\n","    #sum of freq per row\n","    total_freqs = list(np.array(f.sum(axis=0))[0])\n","\n","    #put vocab and frew to feed Wordcloud\n","    fdict = dict(zip(vocab,total_freqs))\n","    # generate word cloud of words with highest counts\n","    wordcloud = WordCloud().generate_from_frequencies(fdict)\n","    print(topic_id)\n","    plt.clf()\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.show()"]},{"cell_type":"markdown","source":["\n","\n","> Use the replication material to build a Political Science Topic Classifier\n","\n","[Topic Classification for Political Texts](https://www.cambridge.org/core/journals/political-analysis/article/crossdomain-topic-classification-for-political-texts/F074564984969CE168BCBCF5E7D931C8#article)\n","\n","\n","*What could they have done different in the pre process step?*"],"metadata":{"id":"GJK3H5MGJFRa"}},{"cell_type":"code","source":["#### YOUR CODE STARTS HERE ####\n","\n","\n"," ...\n","\n","\n","\n","\n","##### YOUR CODE ENDS HERE ####"],"metadata":{"id":"qc50vtC9JZKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9siGHb1x3mz"},"outputs":[],"source":["#This was in 2019 (print the metrics)\n","metrics"]},{"cell_type":"code","source":["#This was in 2023\n","# https://manifesto-project.wzb.eu/information/documents/manifestoberta\n"],"metadata":{"id":"PQKMyozRLKTz"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO+K1c8sxApPD1tEhcvN4R/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}