{"cells":[{"cell_type":"markdown","metadata":{"id":"FFXALOAAFDHP"},"source":["# HW1: Classification with Machine Learning and Deep Learning\n","\n","You will build a classifier to recognize  districs based on the fishing profile. This assignment will step you through how to do this with a Neural Network mindset.\n","\n","**Instructions:**\n","- Add you code in the cells with\n","\n","\n","```\n","### START CODE HERE ###\n","```\n","\n","\"## YOUR CODE HERE\"\n","\n","**You will learn to:**\n","- Build the general architecture of a learning algorithm, including:\n","    - Initializing parameters\n","    - Calculating the cost function and its gradient\n","    - Using an optimization algorithm (gradient descent)\n","- Gather all three functions above into a main model function, in the right order.\n"]},{"cell_type":"markdown","metadata":{"id":"IkQBQbzBFDHU"},"source":["\n","## Mount and Packages ##\n","\n","First, let's run the cell below to import all the packages that you will need during this assignment.\n","- [numpy](https://numpy.org/doc/1.20/) is the fundamental package for scientific computing with Python.\n","- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Rw675vfRFDHV"},"outputs":[],"source":["### v1.2"]},{"cell_type":"code","source":["#Import drive\n","from google.colab import drive\n","#Mount Google Drive\n","ROOT=\"/content/drive\"\n","drive.mount(ROOT, force_remount=True)"],"metadata":{"id":"h6aFKWlGIuA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaGsYEDpFDHW"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n"]},{"cell_type":"markdown","metadata":{"id":"PDa37guSFDHW"},"source":["\n","## Overview of the Problem set ##\n","\n","**Problem Statement**: You are given a dataset (\"SalmonandSeaTroutNets1952-2022.csv\")\n","You will build two algorithms, answering the following quetsions:\n","- Can the fishing profile of the selected columns (selected_columns) of the dataset characterize the top four districts? (Top four districts means those with more fishing records, aka more data samples (not necessarily more weight or fish caught))\n","- Can a simple Neural Network classify better the district with lower metrics in the first model? (This will be a binary classification, keeping the model data from the first)\n","\n","The steps will hint you into the type of problem and model to apply"]},{"cell_type":"code","source":["selected_columns = ['District',\n","'Month',\n","'Wild MSW number',\n","'Wild MSW weight (kg)',\n","'Wild 1SW number',\n","'Wild 1SW weight (kg)',\n","'Sea trout number',\n","'Sea trout weight (kg)',\n","'Finnock number',\n","'Finnock weight (kg)',\n","'Farmed MSW number',\n","'Farmed MSW weight (kg)',\n","'Farmed 1SW number']"],"metadata":{"id":"RZa885ZF_vjF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Description:\n","\n","https://www.kaggle.com/datasets/mikhail1681/salmon-catch-statistics-for-scotland-19522022\n","\n","\n","**District**:\n","\n","**District ID**: Numerical Salmon Fishery District identifier\n","\n","Report order: Spatial ordering of Reporting Areas\n","\n","**Region**: Salmon Fishery Region\n","\n","**Method**: Fishing method\n","\n","**Year**: Year of season fish were reported caught\n","\n","**Month**: Month fish were reported caught\n","\n","Month number: Number of month that fish were reported caught\n","\n","**Wild MSW numbe**r: Number of wild multi sea-winter salmon reported caught\n","\n","Wild MSW weight (kg): Weight of wild multi sea-winter salmon reported caught\n","\n","**Wild 1SW number**: Number of wild one sea-winter salmon reported caught\n","\n","Wild 1SW weight (kg): Weight of wild one sea-winter salmon reported caught\n","\n","**Sea trout number**: Number of sea trout reported caught\n","\n","Sea trout weight (kg): Weight of sea trout reported caught\n","\n","**Finnock number**: Number of finnock reported caught\n","\n","Finnock weight (kg): Weight of finnock reported caught\n","\n","**Farmed MSW number**: Number of farmed-origin multi sea-winter salmon reported caught\n","\n","Farmed MSW weight (kg): Weight of farmed-origin multi sea-winter salmon reported caught\n","\n","**Farmed 1SW number**: Number of farmed-origin one sea-winter salmon reported caught\n","\n","Farmed 1SW weight (kg): Weight of farmed-origin one sea-winter salmon reported caught\n","\n","Netting effort: Summarised as median number of crews/traps"],"metadata":{"id":"zB6O69MARv1U"}},{"cell_type":"markdown","source":["# 1. Data Pre Processing: 1 pts"],"metadata":{"id":"lXApcrXD_aKG"}},{"cell_type":"markdown","source":["## 1.1 Read the file you upload into your environment (you can download the file in OLAT): **0.5 pt**\n"],"metadata":{"id":"v0Qa-1B1LLar"}},{"cell_type":"code","source":["### START CODE HERE ### ( 2 lines) replace \"...\" by your code\n","import ...\n","df= ..."],"metadata":{"id":"kCuukxqrOXrA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cw_rH6E__8kT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2 Select the columns from selected_columns and perform 1 data cleaning step: **0.5 pt**"],"metadata":{"id":"8V1a-LnoLHQ3"}},{"cell_type":"code","source":["# Select the columns from selected_columns and perform 1 data cleaning step: 0.5 pt"],"metadata":{"id":"ggnlODRA_0ep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### START CODE HERE ### ( 2 lines) replace \"...\" by your code\n","df_selected = ...\n","\n","\n","df_cleaned = ..."],"metadata":{"id":"r31Qdz4JADRT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Data Visualization: 0.5 pts"],"metadata":{"id":"BKvLjZLJAgFC"}},{"cell_type":"markdown","source":["## 2.1 Plot an histogram for the number of records in each distric, choose the top four districts with more records.: **0.25 pts**\n","\n","Use that dataset as model input (you should have only 4 districts in your dataset)"],"metadata":{"id":"4AQXQkcbLBPO"}},{"cell_type":"code","source":["### START CODE HERE ###( +2 lines) replace \"...\" by your code\n","..."],"metadata":{"id":"eQXRDR1BAek-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 Choose the top 4 recorded districts: **0.25 pts**"],"metadata":{"id":"5XzyRIJoK7hZ"}},{"cell_type":"code","source":["### START CODE HERE ###( +2 lines) replace \"...\" by your code\n","..."],"metadata":{"id":"um5BDrjXA_3I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Modeling: **1.25 pts**"],"metadata":{"id":"iwIymeVWAo3_"}},{"cell_type":"code","source":[],"metadata":{"id":"86Sqa3kUCMeQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.1 Deal with the categorical column to be able to input the dataset into the model: **0.5 pt**"],"metadata":{"id":"6-wjAdulK0gw"}},{"cell_type":"code","source":["### START CODE HERE ###( 1 lines) replace \"...\" by your code\n","..."],"metadata":{"id":"R2srYgURANyD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 Slipt test and train dataset: **0.5 pt**"],"metadata":{"id":"B57fmKoxKxC6"}},{"cell_type":"code","source":["### START CODE HERE ### ( 2 lines) replace \"...\" by your code\n","..."],"metadata":{"id":"yPRMPde1BZ96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.3 Run the model chosen according to the problem at hands and Print relevant metrics: **0.25 pts**"],"metadata":{"id":"xUCqiA74KspS"}},{"cell_type":"code","source":["### START CODE HERE ### replace \"...\" by your code\n","..."],"metadata":{"id":"t_zjadT1IJ6G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"soPYheJOBusZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Expected Output**:\n","\n","<table style=\"width:35%\">\n","  <tr>\n","    <th>Class</th>\n","    <th>Precision</th>\n","    <th>Recall</th>\n","    <th>F1-score</th>\n","    <th>Support</th>\n","  </tr>\n","  <tr>\n","    <td>Cree</td>\n","    <td>0.73</td>\n","    <td>0.73</td>\n","    <td>0.73</td>\n","    <td>132</td>\n","  </tr>\n","  <tr>\n","    <td>Nith</td>\n","    <td>0.66</td>\n","    <td>0.73</td>\n","    <td>0.69</td>\n","    <td>151</td>\n","  </tr>\n","  <tr>\n","    <td>North</td>\n","    <td>0.78</td>\n","    <td>0.77</td>\n","    <td>0.78</td>\n","    <td>164</td>\n","  </tr>\n","  <tr>\n","    <td>Tweed</td>\n","    <td>0.83</td>\n","    <td>0.77</td>\n","    <td>0.80</td>\n","    <td>214</td>\n","  </tr>\n","</table>\n"],"metadata":{"id":"SCSEQBnQBt1k"}},{"cell_type":"markdown","source":["# 4. Interpretation Questions: **2 pts**\n","\n","1.   What is the District with the lowest false positive rate?\n","2.   What is the District with the highest sensivity?\n","3.   How is the model performing and why?"],"metadata":{"id":"4XmhdDH5B7rd"}},{"cell_type":"code","source":["## YOUR ANSWERS HERE\n","1.\n","2.\n","3."],"metadata":{"id":"2k4KfEteIML9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rmVm3cEFDHZ"},"source":["\n","## 5. Deep Learning Model: General Architecture of the learning algorithm ##\n","\n","It's time to design a simple algorithm to distinguish different districts based on the Salmon fishing profile.\n","\n","We will use a binary problem to simplify the implementation. The problem will be distinguishing **Nith** district from all the others. *In a binary setting, 1 is Nith district, 0 is not-Nith district.*\n","\n","You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**: https://www.google.com/url?sa=i&url=https%3A%2F%2Fjamesmccaffrey.wordpress.com%2F2018%2F07%2F07%2Fwhy-a-neural-network-is-always-better-than-logistic-regression%2F&psig=AOvVaw2k6v5Vx9iVUZpaCuJT4zdV&ust=1711113328912000&source=images&cd=vfe&opi=89978449&ved=2ahUKEwiNjbKCuIWFAxVF7gIHHfiGDmYQjRx6BAgAEBY\n","\n","**Mathematical expression of the algorithm**:\n","\n","For one example $x^{(i)}$:\n","$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n","$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$\n","$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n","\n","The cost is then computed by summing over all training examples:\n","$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n","\n","**Key steps**:\n","In this exercise, you will carry out the following steps:\n","\n","    - Initialize the parameters of the model\n","    - Learn the parameters for the model by minimizing the cost  \n","    - Use the learned parameters to make predictions (on the test set)\n","    - Analyse the results and conclude\n","\n","\n","This is a long exercise, the goal of it is for you to follow step by step and understand every part of it (if not, bring your questions to the class). This is a guide for it, which should make its completion faster: https://dennybritz.com/posts/wildml/implementing-a-neural-network-from-scratch/\n","\n","The main algorithm of deep learning is backpropagation, which will be covered in the next theoretical class. This exercise aims to prepare you for the theoretical part."]},{"cell_type":"markdown","metadata":{"id":"uKjsSnOBFDHZ"},"source":["\n","## Building the parts of our algorithm ##\n","\n","The main steps for building a Neural Network are:\n","1. Define the model structure (such as number of input features)\n","2. Initialize the model's parameters\n","3. Loop:\n","    - Calculate current loss (forward propagation)\n","    - Calculate current gradient (backward propagation)\n","    - Update parameters (gradient descent)\n","\n","You often build 1-3 separately and integrate them into one function we call `model()`.\n","\n","### Helper functions\n","\n","\n","### Exercise 1 - sigmoid **ADD CODE HERE**\n","Using your code from \"Python Basics\", implement `sigmoid()`. As you've seen in the figure above, you need to compute $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions. Use np.exp()."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"239ab1cf1028b721fd14f31b8103c40d","grade":false,"grade_id":"cell-520521c430352f3b","locked":false,"schema_version":3,"solution":true,"task":false},"id":"guzDxJqsFDHZ"},"outputs":[],"source":["# GRADED FUNCTION: sigmoid\n","\n","def sigmoid(z):\n","    \"\"\"\n","    Compute the sigmoid of z\n","\n","    Arguments:\n","    z -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(z)\n","    \"\"\"\n","\n","    ### START CODE HERE ### (≈ 2 line of code)\n","    # clip the values of z between -500 and 500\n","    z_clipped = ...\n","\n","    #return the formula using z_clipped\n","    return ...\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JqYSHElZFDHa"},"source":["\n","### Initializing parameters\n","\n","\n","### Exercise 2 - initialize_with_zeros **ADD CODE HERE**\n","Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"c4a37e375a85ddab7274a33abf46bb7c","grade":false,"grade_id":"cell-befa9335e479864e","locked":false,"schema_version":3,"solution":true,"task":false},"id":"U0U7FNILFDHa"},"outputs":[],"source":["# GRADED FUNCTION: initialize_with_zeros\n","\n","def initialize_with_zeros(X):\n","    \"\"\"\n","    This function creates a vector of zeros of shape (num_features, 1) for w and initializes b to 0.\n","\n","    Argument:\n","    dim -- size of the w vector we want (or number of parameters in this case)\n","\n","    Returns:\n","    weights -- initialized vector of shape (num_features, 1)\n","    bias -- initialized scalar (corresponds to the bias)\n","    \"\"\"\n","\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    # Initialize parameters\n","    num_samples, num_features = X.shape\n","    weights = ... .reshape(-1,1) #leave the reshape\n","    bias = 0.0\n","    ### END CODE HERE ###\n","\n","    assert(weights.shape == (num_features, 1))\n","    assert(isinstance(bias, float) or isinstance(bias, int))\n","\n","    return weights, bias"]},{"cell_type":"markdown","metadata":{"id":"Z_pwkTsdFDHa"},"source":["\n","### Forward and Backward propagation\n","\n","Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n","\n","\n","### Exercise 3 - propagate **ADD CODE HERE**\n","Implement a function `propagate()` that computes the cost function and its gradient.\n","\n","**Hints**:\n","\n","Forward Propagation:\n","- You get X\n","- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n","- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n","\n","Here are the two formulas you will be using:\n","\n","$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n","$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"]},{"cell_type":"code","source":["def compute_cost( Y, y_predicted):\n","\n","        epsilon = 1e-15  # Small constant to avoid division by zero\n","        num_samples = len(Y)\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        # replace the \"...\" by the correct variable\n","        cost = (-1/num_samples) * np.sum(Y * np.log(... + epsilon) + (1 - Y) * np.log(1 - ... + epsilon))\n","\n","        return cost"],"metadata":{"id":"sdFE8yrb75_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"8552b2c9cff2b5fa537fab9f98a6e4da","grade":false,"grade_id":"cell-11af17e28077b3d3","locked":false,"schema_version":3,"solution":true,"task":false},"id":"HEEkJvPQFDHb"},"outputs":[],"source":["# GRADED FUNCTION: propagate\n","\n","def propagate(weights, bias, X, Y):\n","    \"\"\"\n","    Implement the cost function and its gradient for the propagation explained above\n","\n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px * 3, number of examples)\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n","\n","    Return:\n","    cost -- negative log-likelihood cost for logistic regression\n","    dw -- gradient of the loss with respect to w, thus same shape as w\n","    db -- gradient of the loss with respect to b, thus same shape as b\n","\n","    Tips:\n","    - Write your code step by step for the propagation. np.log(), np.dot()\n","    \"\"\"\n","    # Assertions for dimensions\n","    assert X.shape[0] == Y.shape[0], \"Number of samples in X and y must match\"\n","    num_samples, num_features = X.shape\n","### START CODE HERE ### (≈ 2 line of code)\n","  # FORWARD PROPAGATION (FROM X TO COST)\n","    linear_model = ...\n","    y_predicted = ...\n","\n","  # Compute and save cost\n","    cost = compute_cost(Y, y_predicted)\n","\n","### START CODE HERE ### (≈ 2 line of code)\n","    # BACKWARD PROPAGATION (TO FIND GRADIENT)\n","    dw = (1 / num_samples) * np.dot(...)\n","    db = (1 / num_samples) * np.sum(...)\n","\n","\n","    assert(dw.shape == weights.shape)\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","\n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","\n","    return grads, cost"]},{"cell_type":"markdown","metadata":{"id":"DjKeTSKfFDHb"},"source":["\n","### Optimization\n","- You have initialized your parameters.\n","- You are also able to compute a cost function and its gradient.\n","- Now, you want to update the parameters using gradient descent.\n","\n","### Exercise 4 - optimize **ADD CODE HERE**\n","Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"49d9b4c1a780bf141c8eb48e06cbb494","grade":false,"grade_id":"cell-616d6883e807448d","locked":false,"schema_version":3,"solution":true,"task":false},"id":"GQlveGizFDHb"},"outputs":[],"source":["# GRADED FUNCTION: optimize\n","\n","def optimize(weights, bias, X, Y, num_iterations, learning_rate):\n","    \"\"\"\n","    This function optimizes w and b by running a gradient descent algorithm\n","\n","    Arguments:\n","    weights -- weights, a numpy array of size (1, num_features)\n","    bias -- bias, a scalar\n","    X -- data of shape (number of examples, num_features)\n","    Y -- true \"label\" vector (containing 0 if non-nith, 1 if nith), of shape (number of examples, 1)\n","    num_iterations -- number of iterations of the optimization loop\n","    learning_rate -- learning rate of the gradient descent update rule\n","\n","\n","    Returns:\n","    params -- dictionary containing the weights w and bias b\n","    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n","    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n","\n","    Tips:\n","    You basically need to write down two steps and iterate through them:\n","        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n","        2) Update the parameters using gradient descent rule for w and b.\n","    \"\"\"\n","    costs_history=[]\n","    # Gradient descent\n","    for i in range(num_iterations):\n","\n","\n","        # Cost and gradient calculation (≈ 1-4 lines of code)\n","        ### START CODE HERE ###\n","        grads, cost = ...\n","        ### END CODE HERE ###\n","\n","        # Retrieve derivatives from grads\n","        dw = ...\n","        db = ...\n","\n","        # update rule (≈ 2 lines of code)\n","        ### START CODE HERE ###\n","        weights = ...\n","        bias = ...\n","        ### END CODE HERE ###\n","\n","        # Record the costs\n","        if i % 100 == 0:\n","            costs_history.append(cost)\n","\n","            # Print the cost every 100 training examples\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","\n","    params = {\"w\": weights,\n","                \"b\": bias}\n","\n","    grads = {\"dw\": dw,\n","              \"db\": db}\n","\n","    return params, grads, costs_history"]},{"cell_type":"markdown","metadata":{"id":"a3RysIGJFDHc"},"source":["\n","### Exercise 5 - predict **ADD CODE HERE**\n","The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:\n","\n","1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n","\n","2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this)."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"e56419b97ebf382a8f93ac2873988887","grade":false,"grade_id":"cell-d6f924f49c51dc2f","locked":false,"schema_version":3,"solution":true,"task":false},"id":"MoR9ucZlFDHc"},"outputs":[],"source":["# GRADED FUNCTION: predict\n","\n","def predict(weights, bias, X):\n","    '''\n","    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n","\n","    Arguments:\n","    weights -- weights, a numpy array of size (num_samples, 1)\n","    bias -- bias, a scalar\n","    X -- data of size (num_samples, num_features)\n","\n","    Returns:\n","    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n","    '''\n","    # Assertion for dimensions\n","    assert X.shape[1] == len(weights), \"Number of features in X must match the size of weights vector\"\n","### START CODE HERE ###\n","    linear_model = ...\n","    y_predicted = ...\n","    y_predicted_cls = ...\n"," ### END CODE HERE ###\n","\n","    return y_predicted_cls"]},{"cell_type":"markdown","metadata":{"id":"fly7kbNWFDHd"},"source":["<font color='blue'>\n","    \n","**What to remember:**\n","    \n","You've implemented several functions that:\n","- Initialize (w,b)\n","- Optimize the loss iteratively to learn parameters (w,b):\n","    - Computing the cost and its gradient\n","    - Updating the parameters using gradient descent\n","- Use the learned (w,b) to predict the labels for a given set of examples"]},{"cell_type":"markdown","metadata":{"id":"s5H43NQsFDHd"},"source":["\n","## Merge all functions into a model ##\n","\n","You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n","\n","\n","\n","## Exercise 6 - model **ADD CODE HERE**\n","Implement the model function. Use the following notation:\n","    - Y_prediction_test for your predictions on the test set\n","    - Y_prediction_train for your predictions on the train set\n","    - parameters, grads, costs for the outputs of optimize()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"b62adfb8f5a0f5bb5aa6798c3c5df66d","grade":false,"grade_id":"cell-6dcba5967c4cbf8c","locked":false,"schema_version":3,"solution":true,"task":false},"id":"8w0MHYWqFDHd"},"outputs":[],"source":["# GRADED FUNCTION: model\n","\n","def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5):\n","    \"\"\"\n","    Builds the logistic regression model by calling the function you've implemented previously\n","\n","    Arguments:\n","    X_train -- training set represented by a numpy array of shape (num_samples_train, num_features)\n","    Y_train -- training labels represented by a numpy array (vector) of shape (num_samples_train, num_features)\n","    X_test -- test set represented by a numpy array of shape (num_samples_test, num_features)\n","    Y_test -- test labels represented by a numpy array (vector) of shape (num_samples_test, 1)\n","    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n","    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n","\n","\n","    Returns:\n","    d -- dictionary containing information about the model.\n","    \"\"\"\n","    #Y_train=y_train\n","    #Y_test=y_test\n","    #num_iterations = 2000\n","    #learning_rate = 0.5\n","\n","    ### START CODE HERE ###\n","\n","    # initialize parameters with zeros (≈ 1 line of code)\n","\n","    weights, bias = ...\n","\n","    # Gradient descent (≈ 1 line of code)\n","    parameters, grads, costs_history =  ...\n","\n","    # Retrieve parameters w and b from dictionary \"parameters\"\n","    weights = ...\n","    bias = ...\n","\n","    # Predict test/train set examples (≈ 2 lines of code)\n","    Y_prediction_test = ...\n","    Y_prediction_train = ...\n","\n","    ### END CODE HERE ###\n","\n","    # Print train/test Errors\n","\n","    accuracy_test = np.mean(Y_prediction_test == Y_test.reshape(1,-1))\n","    print(\"Test Accuracy:\", accuracy_test)\n","\n","    accuracy_train = np.mean(Y_prediction_train == Y_train.reshape(1,-1))\n","    print(\"Test Accuracy:\", accuracy_train)\n","\n","    # Plot cost over iterations\n","    import matplotlib.pyplot as plt\n","    plt.plot(range(len(costs_history)), costs_history)\n","    plt.xlabel('Iterations')\n","    plt.ylabel('Cost')\n","    plt.title('Cost vs. Iterations')\n","    plt.show()\n","\n","    d = {\"costs\": costs_history,\n","          \"Y_prediction_test\": Y_prediction_test,\n","          \"Y_prediction_train\" : Y_prediction_train,\n","          \"w\" : weights,\n","          \"b\" : bias,\n","          \"learning_rate\" : learning_rate,\n","          \"num_iterations\": num_iterations}\n","\n","    return d"]},{"cell_type":"markdown","source":["# 5.1 Apply to your dataset: 1.25"],"metadata":{"id":"R0RJPU7MEHG8"}},{"cell_type":"markdown","source":["## 5.2 Replace the Nith by 1 and the remaining 3 districts by 0 in your y column (or label column): **0.05 pts**"],"metadata":{"id":"TsvNCb0IEo9N"}},{"cell_type":"code","source":["### START CODE HERE ###\n","df_dummies_model['District'] = ...\n","df_dummies_model['y'] = ..."],"metadata":{"id":"OH6P4DyfOAtG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.3 Do a train and test split: **0.05 pts**"],"metadata":{"id":"rtRQfUcMKCqb"}},{"cell_type":"code","source":["### START CODE HERE ###\n","X_train, X_test, y_train, y_test = ..."],"metadata":{"id":"pJow9wiCN_8A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.4 Run the model in your data: **1.15 pts**"],"metadata":{"id":"suURNiK_J6Sw"}},{"cell_type":"code","source":["dict_data=model(X_train.values, y_train.values, X_test.values, y_test.values, num_iterations = 1000, learning_rate = 0.1) #this code is given, you are graded if it works\n","\n"],"metadata":{"id":"PvG0D7l9y9JB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#HOW TO DELIVER: https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab\n","#download this notebook after all your answers and upload it to the files tab on the left\n","#right click on it to check the path\n","#replace the path ('/content/Untitled0.ipynb') by your file path\n","%%shell\n","jupyter nbconvert --to html /content/Untitled0.ipynb\n","\n","#refresh the folder (not the page)\n","#download the html"],"metadata":{"id":"HbKDwKMJGkty"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}